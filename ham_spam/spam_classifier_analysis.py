#%%
import numpy as np
import jieba
import re
import string
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå…¨å±€é£æ ¼
plt.rcParams['font.sans-serif'] = ['SimHei']  # æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False     # è´Ÿå·æ­£å¸¸æ˜¾ç¤º
sns.set(style="whitegrid", font="SimHei")

#%%
def get_data():
    with open("./data/ham_data.txt", encoding="utf8") as ham_f, open("./data/spam_data.txt", encoding="utf8") as spam_f:
        ham_data = ham_f.readlines()
        spam_data = spam_f.readlines()
        ham_label = np.ones(len(ham_data)).tolist()
        spam_label = np.zeros(len(spam_data)).tolist()
        corpus = ham_data + spam_data
        labels = ham_label + spam_label
    return corpus, labels

#%%
def remove_empty_docs(corpus, labels):
    corpus_clean, labels_clean = [], []
    for doc, label in zip(corpus, labels):
        if doc.strip():
            corpus_clean.append(doc)
            labels_clean.append(label)
    return corpus_clean, labels_clean

#%%
def tokenize_text(text):
    tokens = jieba.cut(text)
    return [t.strip() for t in tokens]

def remove_special_characters(text):
    tokens = tokenize_text(text)
    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))
    clean_tokens = [pattern.sub('', token) for token in tokens if token.strip()]
    return ' '.join(clean_tokens)

with open("./data/stop_words.utf8", encoding="utf8") as f:
    stopwords = f.readlines()

def remove_stopwords(text):
    tokens = tokenize_text(text)
    clean_tokens = [t for t in tokens if t not in stopwords]
    return ''.join(clean_tokens)

def normalize_corpus(corpus):
    normalized = []
    for text in corpus:
        text = remove_special_characters(text)
        text = remove_stopwords(text)
        normalized.append(text)
    return normalized

#%%
def bow_extractor(corpus):
    vectorizer = CountVectorizer(min_df=1)
    features = vectorizer.fit_transform(corpus)
    return vectorizer, features

def tfidf_extractor(corpus):
    vectorizer = TfidfVectorizer(min_df=1, norm='l2', smooth_idf=True)
    features = vectorizer.fit_transform(corpus)
    return vectorizer, features

#%%
corpus, labels = get_data()
print("æ€»çš„æ•°æ®é‡:", len(labels))
corpus, labels = remove_empty_docs(corpus, labels)
train_corpus, test_corpus, train_labels, test_labels = train_test_split(corpus, labels, test_size=0.3, random_state=42)

#%%
norm_train_corpus = normalize_corpus(train_corpus)
norm_test_corpus = normalize_corpus(test_corpus)

bow_vectorizer, bow_train_features = bow_extractor(norm_train_corpus)
bow_test_features = bow_vectorizer.transform(norm_test_corpus)

tfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)
tfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)

#%%
mnb_bow = MultinomialNB().fit(bow_train_features, train_labels)
mnb_tfidf = MultinomialNB().fit(tfidf_train_features, train_labels)
lr_tfidf = LogisticRegression(max_iter=1000).fit(tfidf_train_features, train_labels)

#%%
print("åŸºäºè¯è¢‹æ¨¡å‹çš„å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯æ¨¡å‹")
print("è®­ç»ƒé›†å¾—åˆ†ï¼š", mnb_bow.score(bow_train_features, train_labels))
print("æµ‹è¯•é›†å¾—åˆ†ï¼š", mnb_bow.score(bow_test_features, test_labels))

print("åŸºäºtfidfçš„å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯æ¨¡å‹")
print("è®­ç»ƒé›†å¾—åˆ†ï¼š", mnb_tfidf.score(tfidf_train_features, train_labels))
print("æµ‹è¯•é›†å¾—åˆ†ï¼š", mnb_tfidf.score(tfidf_test_features, test_labels))

print("åŸºäºtfidfçš„é€»è¾‘å›å½’æ¨¡å‹")
print("è®­ç»ƒé›†å¾—åˆ†ï¼š", lr_tfidf.score(tfidf_train_features, train_labels))
print("æµ‹è¯•é›†å¾—åˆ†ï¼š", lr_tfidf.score(tfidf_test_features, test_labels))

#%%
# ===================== ğŸ“Š å­¦æœ¯å¯è§†åŒ–åˆ†ææ¨¡å— =====================

def model_report(name, model, X_test, y_test):
    print(f"\nğŸ“˜ æ¨¡å‹åˆ†ææŠ¥å‘Šï¼š{name}")
    y_pred = model.predict(X_test)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    print(f"ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰: {precision:.4f}")
    print(f"å¬å›ç‡ï¼ˆRecallï¼‰: {recall:.4f}")
    print(f"F1 å€¼: {f1:.4f}")

    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, square=True)
    plt.title(f'{name} æ··æ·†çŸ©é˜µ', fontsize=14, fontweight='bold')
    plt.xlabel('é¢„æµ‹ç±»åˆ«', fontsize=12)
    plt.ylabel('çœŸå®ç±»åˆ«', fontsize=12)
    plt.tight_layout()
    plt.savefig(f'{name}_æ··æ·†çŸ©é˜µ.png', dpi=300)
    plt.close()

# æ¨¡å‹æŠ¥å‘Š
model_report("BoW+NaiveBayes", mnb_bow, bow_test_features, test_labels)
model_report("TFIDF+NaiveBayes", mnb_tfidf, tfidf_test_features, test_labels)
model_report("TFIDF+LogisticRegression", lr_tfidf, tfidf_test_features, test_labels)

#%%
# ğŸ“ˆ å„æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”
models = ['BoW+NB', 'TFIDF+NB', 'TFIDF+LR']
train_scores = [
    mnb_bow.score(bow_train_features, train_labels),
    mnb_tfidf.score(tfidf_train_features, train_labels),
    lr_tfidf.score(tfidf_train_features, train_labels)
]
test_scores = [
    mnb_bow.score(bow_test_features, test_labels),
    mnb_tfidf.score(tfidf_test_features, test_labels),
    lr_tfidf.score(tfidf_test_features, test_labels)
]

x = np.arange(len(models))
width = 0.35
plt.figure(figsize=(7,5))
plt.bar(x - width/2, train_scores, width, label='è®­ç»ƒé›†', color='#4A90E2')
plt.bar(x + width/2, test_scores, width, label='æµ‹è¯•é›†', color='#F5A623')
plt.xticks(x, models, fontsize=11)
plt.ylabel('å‡†ç¡®ç‡', fontsize=12)
plt.title('å„æ¨¡å‹è®­ç»ƒ/æµ‹è¯•å‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
plt.legend()
plt.tight_layout()
plt.savefig('æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”æŸ±çŠ¶å›¾.png', dpi=300)
plt.close()

#%%
# ROCæ›²çº¿æ¯”è¾ƒ
plt.figure(figsize=(6,5))
for name, model, X in [
    ("BoW+NB", mnb_bow, bow_test_features),
    ("TFIDF+NB", mnb_tfidf, tfidf_test_features),
    ("TFIDF+LR", lr_tfidf, tfidf_test_features)
]:
    y_prob = model.predict_proba(X)[:,1]
    fpr, tpr, _ = roc_curve(test_labels, y_prob)
    auc_value = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC={auc_value:.3f})')

plt.plot([0,1],[0,1],'k--',lw=1)
plt.xlabel('å‡é˜³ç‡ (FPR)', fontsize=12)
plt.ylabel('çœŸé˜³ç‡ (TPR)', fontsize=12)
plt.title('æ¨¡å‹ROCæ›²çº¿æ¯”è¾ƒ', fontsize=14, fontweight='bold')
plt.legend()
plt.tight_layout()
plt.savefig('æ¨¡å‹ROCæ›²çº¿æ¯”è¾ƒ.png', dpi=300)
plt.close()

#%%
# â˜ï¸ é«˜é¢‘è¯äº‘ï¼ˆåŒºåˆ†è‰²è°ƒï¼‰
def plot_wordcloud(corpus, title, filename, color):
    text = ' '.join(corpus)
    wc = WordCloud(
        font_path='simhei.ttf',
        background_color='white',
        width=800, height=600,
        colormap=color,
        max_words=200
    ).generate(text)
    plt.figure(figsize=(8,6))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(title, fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(filename, dpi=300)
    plt.close()

ham_texts = [norm_train_corpus[i] for i, label in enumerate(train_labels) if label == 1]
spam_texts = [norm_train_corpus[i] for i, label in enumerate(train_labels) if label == 0]

plot_wordcloud(ham_texts, "æ­£å¸¸çŸ­ä¿¡é«˜é¢‘è¯äº‘", "æ­£å¸¸çŸ­ä¿¡é«˜é¢‘è¯äº‘.png", "Blues")
plot_wordcloud(spam_texts, "åƒåœ¾çŸ­ä¿¡é«˜é¢‘è¯äº‘", "åƒåœ¾çŸ­ä¿¡é«˜é¢‘è¯äº‘.png", "Oranges")

print("\nâœ… å¯è§†åŒ–ç»“æœå·²ç”Ÿæˆï¼ŒåŒ…å«ï¼š")
print(" - å„æ¨¡å‹æ··æ·†çŸ©é˜µï¼ˆè“è‰²æ–¹æ ¼ï¼‰")
print(" - æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”æŸ±çŠ¶å›¾.png")
print(" - æ¨¡å‹ROCæ›²çº¿æ¯”è¾ƒ.png")
print(" - æ­£å¸¸/åƒåœ¾çŸ­ä¿¡é«˜é¢‘è¯äº‘.png")
print("æ‰€æœ‰å›¾åƒå·²ä¼˜åŒ–ä¸ºè®ºæ–‡/æ±‡æŠ¥é£æ ¼ã€‚")
